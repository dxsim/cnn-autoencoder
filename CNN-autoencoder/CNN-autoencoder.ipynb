{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input,Dense,Activation,BatchNormalization,Flatten,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from keras.layers import MaxPooling2D, Dropout, UpSampling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for dirs\n",
    "\n",
    "train_dir = './data/train'\n",
    "test_dir = './data/test'\n",
    "model_dir = './models'\n",
    "\n",
    "for dir in [train_dir,test_dir,model_dir]:\n",
    "    if not(os.path.exists(dir)):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading info from parser\n",
    "\n",
    "info = {\n",
    "    \"inputShape\": (200,200,1),\n",
    "    \"autoencoderFile\": os.path.join(model_dir, \"autoecoder.h5\"),\n",
    "    \"encoderFile\": os.path.join(model_dir, \"encoder.h5\"),\n",
    "    \"decoderFile\": os.path.join(model_dir, \"decoder.h5\"),\n",
    "    \"checkpointFile\": os.path.join(model_dir, \"checkpoint.h5\"),\n",
    "    \"trainHistory\": os.path.join(model_dir, \"train_history.csv\"),\n",
    "    \"mode\": 'train',\n",
    "    \"retrain\": True,\n",
    "    \"loss\": 'binary_crossentropy',\n",
    "    \"optimizer\": 'adam',\n",
    "    \"batchSize\": 32,\n",
    "    \"multiprocessing\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 images belonging to 7 classes.\n",
      "Found 58 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\n",
    "# flow_from_directory\n",
    "# https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n",
    "\n",
    "# ImageDataGenerator and flow_from_directory\n",
    "# View documentation: https://keras.io/preprocessing/image/\n",
    "train_datagen = image.ImageDataGenerator(rescale=1./255,\n",
    "                                    validation_split=0.2\n",
    "                                  )\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True\n",
    "            \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=info[\"inputShape\"][:2],\n",
    "    batch_size=info[\"batchSize\"],\n",
    "    color_mode='grayscale',\n",
    "    class_mode='input',\n",
    "    subset='training',\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=info[\"inputShape\"][:2],\n",
    "    batch_size=info[\"batchSize\"],\n",
    "    color_mode='grayscale',\n",
    "    class_mode='input',\n",
    "    subset='validation',\n",
    "    seed=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_generator.samples//32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    \n",
    "    def __init__(self, info):\n",
    "        self.info= info\n",
    "\n",
    "    \n",
    "    def build_models(self):\n",
    "        \n",
    "        # Build and compile\n",
    "        # Print building models\n",
    "        conv_shape = (3,3) # convolutional kernel shape\n",
    "        pool_shape = (2,2) # pooling kernel shape\n",
    "        n_hidden_1, n_hidden_2, n_hidden_3 = 16, 8, 8 # channel numbers\n",
    "        input_shape = self.info['inputShape']\n",
    "        input_layer = Input(shape= input_shape)\n",
    "        \n",
    "        #encoder layers\n",
    "        x = Conv2D(n_hidden_1, conv_shape, activation='relu', padding='same')(input_layer)\n",
    "        x = MaxPooling2D(pool_shape, padding='same')(x)\n",
    "        x = Conv2D(n_hidden_2, conv_shape, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D(pool_shape, padding='same')(x)\n",
    "        x = Conv2D(n_hidden_3, conv_shape, activation='relu', padding='same')(x)\n",
    "        encoded = MaxPooling2D(pool_shape, padding='same')(x)\n",
    "        \n",
    "        #decoded layers\n",
    "        x = Conv2D(n_hidden_3, conv_shape, activation='relu', padding='same')(encoded)\n",
    "        x = UpSampling2D(pool_shape)(x)\n",
    "        x = Conv2D(n_hidden_2, conv_shape, activation='relu', padding='same')(x)\n",
    "        x = UpSampling2D(pool_shape)(x)\n",
    "        x = Conv2D(n_hidden_1, conv_shape, activation='relu', padding='same')(x)\n",
    "        x = UpSampling2D(pool_shape)(x)\n",
    "        decoded = Conv2D(input_shape[2], conv_shape, activation='sigmoid', padding='same')(x)\n",
    "        \n",
    "        # Creating Autoencoder\n",
    "        autoencoder = Model(input_layer,decoded)\n",
    "        # Creating Encoder\n",
    "        encoder = Model(input_layer,encoded)\n",
    "        \n",
    "        # Output encoder shapes\n",
    "        output_encoder_shape = encoder.layers[-1].output_shape[1:]\n",
    "\n",
    "        # Create decoder model (Reverse)\n",
    "        decoded_input = Input(shape=output_encoder_shape)\n",
    "        \n",
    "        decoded_output = autoencoder.layers[-7](decoded_input)  # Conv2D\n",
    "        decoded_output = autoencoder.layers[-6](decoded_output)  # UpSampling2D\n",
    "        decoded_output = autoencoder.layers[-5](decoded_output)  # Conv2D\n",
    "        decoded_output = autoencoder.layers[-4](decoded_output)  # UpSampling2D\n",
    "        decoded_output = autoencoder.layers[-3](decoded_output)  # Conv2D\n",
    "        decoded_output = autoencoder.layers[-2](decoded_output)  # UpSampling2D\n",
    "        decoded_output = autoencoder.layers[-1](decoded_output)  # Conv2D\n",
    "        \n",
    "        decoder = Model(decoded_input, decoded_output)\n",
    "        \n",
    "        # Generate summaries\n",
    "        print(\"\\nautoencoder.summary():\")\n",
    "        print(autoencoder.summary())\n",
    "        print(\"\\nencoder.summary():\")\n",
    "        print(encoder.summary())\n",
    "        print(\"\\ndecoder.summary():\")\n",
    "        print(decoder.summary())\n",
    "        \n",
    "        self.autoencoder = autoencoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        print('Models succesfully built')\n",
    "        \n",
    "        self.compile(loss = self.info['loss'], optimizer = self.info['optimizer'])\n",
    "        \n",
    "        print('Building and compilation done')\n",
    "        \n",
    "    def compile(self, loss='binary_crossentropy', optimizer='adam'):\n",
    "        print('Compiling models...')\n",
    "        # To fit the model using the parameters\n",
    "        self.autoencoder.compile(loss=loss,optimizer=optimizer)\n",
    "    \n",
    "    def predict_embedding(self,X):\n",
    "        return self.encoder.predict(X)\n",
    "    \n",
    "    def reconstruct_img(self,X):\n",
    "        return self.autoencoder.predict(X)\n",
    "    \n",
    "    def fit(self, train_generator, validation_generator, n_epochs=2, batch_size=256, callbacks=[]): \n",
    "        # Split the train test set\n",
    "        print('Fitting models....')\n",
    "\n",
    "        self.autoencoder.fit_generator(\n",
    "                    train_generator,\n",
    "                    steps_per_epoch = train_generator.samples // batch_size,\n",
    "                    validation_data = validation_generator, \n",
    "                    validation_steps = validation_generator.samples // batch_size,\n",
    "                    verbose=1,\n",
    "                    epochs = n_epochs,\n",
    "                    use_multiprocessing=self.info[\"multiprocessing\"],\n",
    "                    callbacks=callbacks,\n",
    "        )\n",
    "    \n",
    "    def save_models(self):\n",
    "        print('Saving models...')\n",
    "        self.autoencoder.save(self.info[\"autoencoderFile\"])\n",
    "        self.encoder.save(self.info[\"encoderFile\"])\n",
    "        self.decoder.save(self.info[\"decoderFile\"])\n",
    "        \n",
    "        print('models.saved')\n",
    "        \n",
    "    def load_models(self, loss='binary_crossentropy', optimizer='adam'):\n",
    "        print('Loading and compiling models..')\n",
    "        self.autoencoder = load_model(self.info[\"autoencoderFile\"])\n",
    "        self.encoder = load_model(self.info[\"encoderFile\"])\n",
    "        self.decoder = load_model(self.info[\"decoderFile\"])\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "        self.encoder.compile(optimizer=optimizer, loss=loss)\n",
    "        self.decoder.compile(optimizer=optimizer, loss=loss)\n",
    "        \n",
    "        print('Loading and compiling models done')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "\n",
      "autoencoder.summary():\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200, 200, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 16)      160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 8)       1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 25, 25, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 50, 50, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 50, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 100, 100, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 100, 100, 16)      1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 200, 200, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 200, 200, 1)       145       \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "encoder.summary():\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200, 200, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 16)      160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 8)       1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 8)         0         \n",
      "=================================================================\n",
      "Total params: 1,904\n",
      "Trainable params: 1,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "decoder.summary():\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 25, 25, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 25, 25, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 50, 50, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 50, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 100, 100, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 100, 100, 16)      1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 200, 200, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 200, 200, 1)       145       \n",
      "=================================================================\n",
      "Total params: 2,481\n",
      "Trainable params: 2,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Models succesfully built\n",
      "Compiling models...\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Building and compilation done\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(info)\n",
    "\n",
    "\n",
    "if info['retrain']:\n",
    "    model.build_models()\n",
    "else:\n",
    "    if os.path.isfile(info['autoencoderFile']):\n",
    "        model.load_models()\n",
    "    else:\n",
    "        model.build_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_history = info['trainHistory']\n",
    "checkpoint_file = info['checkpointFile']\n",
    "csv_logger=CSVLogger(train_history, append=True, separator=';')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "checkpoint = ModelCheckpoint(checkpoint_file, monitor='val_loss', verbose=0, save_best_only=True, mode='auto', period=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting models....\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\utils\\data_utils.py\", line 666, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\utils\\data_utils.py\", line 661, in <lambda>\n",
      "    initargs=(seqs, self.random_seed))\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 176, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: can't pickle _thread.lock objects\n",
      "\n",
      "Exception in thread Thread-28:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\utils\\data_utils.py\", line 666, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\Lib\\site-packages\\keras\\utils\\data_utils.py\", line 661, in <lambda>\n",
      "    initargs=(seqs, self.random_seed))\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 176, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: can't pickle _thread.lock objects\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "if info['mode']=='train':\n",
    "    hist = model.fit(train_generator,\n",
    "                     validation_generator, \n",
    "                     n_epochs=200,\n",
    "                     batch_size=32,\n",
    "                     callbacks = [csv_logger,es,checkpoint]\n",
    "             )\n",
    "    model.save_models() # Save encoder models\n",
    "\n",
    "    \n",
    "    #csv_logger\n",
    "    # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
